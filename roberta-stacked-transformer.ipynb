{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8473140,"sourceType":"datasetVersion","datasetId":5052582},{"sourceId":8477837,"sourceType":"datasetVersion","datasetId":5056190}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-22T06:07:22.764709Z","iopub.execute_input":"2024-05-22T06:07:22.765161Z","iopub.status.idle":"2024-05-22T06:07:24.343253Z","shell.execute_reply.started":"2024-05-22T06:07:22.765136Z","shell.execute_reply":"2024-05-22T06:07:24.342095Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/global-politics-new-4000/global political data new (1).csv\n/kaggle/input/global-politics/global political data new.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom transformers import RobertaTokenizer, RobertaModel, AdamW\nfrom torch.utils.data import DataLoader, Dataset\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport re\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nimport nltk\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:24.345376Z","iopub.execute_input":"2024-05-22T06:07:24.345973Z","iopub.status.idle":"2024-05-22T06:07:38.152216Z","shell.execute_reply.started":"2024-05-22T06:07:24.345941Z","shell.execute_reply":"2024-05-22T06:07:38.151169Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Download NLTK resources\nnltk.download('punkt')\nnltk.download('stopwords')","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:38.153705Z","iopub.execute_input":"2024-05-22T06:07:38.154279Z","iopub.status.idle":"2024-05-22T06:07:38.416561Z","shell.execute_reply.started":"2024-05-22T06:07:38.154243Z","shell.execute_reply":"2024-05-22T06:07:38.415621Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"# Data Preprocessing\ndef preprocess_text(text):\n    text = text.lower()  # Convert text to lowercase\n    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n    text = re.sub(r'[^a-zA-Z]', ' ', text)  # Remove non-alphabetic characters\n    tokens = word_tokenize(text)  # Tokenize the text\n    stop_words = set(stopwords.words('english'))  # Get English stopwords\n    filtered_tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n    preprocessed_text = ' '.join(filtered_tokens)  # Join tokens back into a string\n    return preprocessed_text","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:38.419486Z","iopub.execute_input":"2024-05-22T06:07:38.420043Z","iopub.status.idle":"2024-05-22T06:07:38.425890Z","shell.execute_reply.started":"2024-05-22T06:07:38.420016Z","shell.execute_reply":"2024-05-22T06:07:38.424956Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Load data\ndata = pd.read_csv(\"/kaggle/input/global-politics-new-4000/global political data new (1).csv\")\n\ndata['tweets'] = data['tweets'].apply(preprocess_text)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:38.427103Z","iopub.execute_input":"2024-05-22T06:07:38.427452Z","iopub.status.idle":"2024-05-22T06:07:40.520310Z","shell.execute_reply.started":"2024-05-22T06:07:38.427422Z","shell.execute_reply":"2024-05-22T06:07:40.519342Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Split data into train and test sets\ntrain_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:40.521413Z","iopub.execute_input":"2024-05-22T06:07:40.521689Z","iopub.status.idle":"2024-05-22T06:07:40.536267Z","shell.execute_reply.started":"2024-05-22T06:07:40.521665Z","shell.execute_reply":"2024-05-22T06:07:40.535329Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#!pip install --upgrade transformers\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:40.537383Z","iopub.execute_input":"2024-05-22T06:07:40.537636Z","iopub.status.idle":"2024-05-22T06:07:40.549666Z","shell.execute_reply.started":"2024-05-22T06:07:40.537615Z","shell.execute_reply":"2024-05-22T06:07:40.548955Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Initialize tokenizer\n\ntokenizer = RobertaTokenizer.from_pretrained('roberta-base')","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:40.550640Z","iopub.execute_input":"2024-05-22T06:07:40.550952Z","iopub.status.idle":"2024-05-22T06:07:41.691956Z","shell.execute_reply.started":"2024-05-22T06:07:40.550921Z","shell.execute_reply":"2024-05-22T06:07:41.691186Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34b67a41b1784d6d92a179f5def053d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fe44fe18d7342c7b35a0a687eaa0bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b13824b56804c2bbaa74a13d4471274"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90a3c9db57744d23ad5fdc01c90a7c2e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e33113f56de439cb2e1bee534cfe361"}},"metadata":{}}]},{"cell_type":"code","source":"# Define CustomDataset class\nclass CustomDataset(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = self.data.tweets.values\n        self.targets = self.data.target.values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            truncation=True,\n            return_token_type_ids=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n        return {\n            'input_ids': torch.tensor(ids, dtype=torch.long),\n            'attention_mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }\n\n# Define Encoder-Decoder model\nclass EncoderDecoder(nn.Module):\n    def __init__(self, num_layers, pretrained_model_name, num_classes, hidden_size):\n        super(EncoderDecoder, self).__init__()\n        self.encoders = nn.ModuleList([RobertaModel.from_pretrained(pretrained_model_name) for _ in range(num_layers)])\n        self.decoders = nn.ModuleList([SentimentDecoder(self.encoders[i].config.hidden_size, hidden_size, num_classes)\n                                        for i in range(num_layers)])\n        self.drop = nn.Dropout(p=0.3)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        pooled_outputs = []\n        for encoder in self.encoders:\n            output = encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            pooled_outputs.append(output.pooler_output)\n        pooled_output = torch.cat(pooled_outputs, dim=1)  # Concatenate pooled outputs from all encoders\n        pooled_output = self.drop(pooled_output)  # Apply dropout\n        decoder_outputs = []\n        for decoder in self.decoders:\n            output, _ = decoder(pooled_output.unsqueeze(0), None)  # Pass pooled_output through each decoder\n            decoder_outputs.append(output)\n        return torch.stack(decoder_outputs).mean(dim=0)  # Average the decoder outputs from all layers\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:41.693070Z","iopub.execute_input":"2024-05-22T06:07:41.693360Z","iopub.status.idle":"2024-05-22T06:07:41.707166Z","shell.execute_reply.started":"2024-05-22T06:07:41.693336Z","shell.execute_reply":"2024-05-22T06:07:41.706329Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class SentimentDecoder(nn.Module):\n    def __init__(self, input_size, hidden_size, output_size):\n        super(SentimentDecoder, self).__init__()\n        self.hidden_size = hidden_size\n        self.gru = nn.GRU(input_size, hidden_size)\n        self.out = nn.Linear(hidden_size, output_size)\n\n    def forward(self, input, hidden):\n        output, hidden = self.gru(input, hidden)\n        output = torch.log_softmax(self.out(output[0]), dim=1)\n        return output, hidden\n\nclass StackedTransformer(nn.Module):\n    def __init__(self, num_layers, pretrained_model_name, num_classes, hidden_size):\n        super(StackedTransformer, self).__init__()\n        self.num_layers = num_layers\n        self.encoders = nn.ModuleList([RobertaModel.from_pretrained(pretrained_model_name) for _ in range(num_layers)])\n        self.decoders = nn.ModuleList([SentimentDecoder(self.encoders[0].config.hidden_size, hidden_size, num_classes) for _ in range(num_layers)])\n        self.drop = nn.Dropout(p=0.3)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        encoder_outputs = []\n        for encoder in self.encoders:\n            encoder_output = encoder(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n            encoder_outputs.append(encoder_output.pooler_output)\n        stacked_encoder_output = torch.stack(encoder_outputs, dim=1)\n        \n        decoder_outputs = []\n        for decoder, encoder_output in zip(self.decoders, stacked_encoder_output.unbind(dim=1)):\n            pooled_output = self.drop(encoder_output)\n            decoder_output, _ = decoder(pooled_output.unsqueeze(0), None)\n            decoder_outputs.append(decoder_output)\n        stacked_decoder_output = torch.stack(decoder_outputs, dim=1)\n        \n        # Combine decoder outputs, e.g., by averaging or concatenation\n        final_output = torch.mean(stacked_decoder_output, dim=1)\n        \n        return final_output.squeeze(0)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:41.710780Z","iopub.execute_input":"2024-05-22T06:07:41.711019Z","iopub.status.idle":"2024-05-22T06:07:41.726881Z","shell.execute_reply.started":"2024-05-22T06:07:41.710999Z","shell.execute_reply":"2024-05-22T06:07:41.726013Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Training parameters\nMAX_LEN = 128\nBATCH_SIZE = 16\nNUM_CLASSES = 2\nLEARNING_RATE = 2e-5\nEPOCHS = 30\nNUM_LAYERS = 4  # Number of encoder-decoder pairs\nHIDDEN_SIZE = 512  # Hidden size of each decoder\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:41.727911Z","iopub.execute_input":"2024-05-22T06:07:41.728535Z","iopub.status.idle":"2024-05-22T06:07:41.743828Z","shell.execute_reply.started":"2024-05-22T06:07:41.728504Z","shell.execute_reply":"2024-05-22T06:07:41.743074Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Usage\nmodel = StackedTransformer(num_layers=4, pretrained_model_name='roberta-base', num_classes=NUM_CLASSES, hidden_size=512)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:41.744901Z","iopub.execute_input":"2024-05-22T06:07:41.745226Z","iopub.status.idle":"2024-05-22T06:07:46.464332Z","shell.execute_reply.started":"2024-05-22T06:07:41.745197Z","shell.execute_reply":"2024-05-22T06:07:46.463617Z"},"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/499M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0f93f067b264ef49e8900323747e7b3"}},"metadata":{}},{"name":"stderr","text":"Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nSome weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"# Move model to appropriate device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:46.465443Z","iopub.execute_input":"2024-05-22T06:07:46.465776Z","iopub.status.idle":"2024-05-22T06:07:47.475390Z","shell.execute_reply.started":"2024-05-22T06:07:46.465744Z","shell.execute_reply":"2024-05-22T06:07:47.474643Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"\n# Initialize optimizer and criterion\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:47.476379Z","iopub.execute_input":"2024-05-22T06:07:47.476631Z","iopub.status.idle":"2024-05-22T06:07:47.490971Z","shell.execute_reply.started":"2024-05-22T06:07:47.476610Z","shell.execute_reply":"2024-05-22T06:07:47.490009Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"\n# Create train and test datasets\ntrain_dataset = CustomDataset(train_data, tokenizer, MAX_LEN)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n\ntest_dataset = CustomDataset(test_data, tokenizer, MAX_LEN)\ntest_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:47.492100Z","iopub.execute_input":"2024-05-22T06:07:47.492504Z","iopub.status.idle":"2024-05-22T06:07:47.498734Z","shell.execute_reply.started":"2024-05-22T06:07:47.492474Z","shell.execute_reply":"2024-05-22T06:07:47.497843Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Training loop\nfor epoch in range(EPOCHS):\n    model.train()\n    epoch_loss = 0\n    correct = 0\n    total = 0\n    for batch_idx, batch in enumerate(train_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        targets = batch['targets'].to(device, dtype=torch.long)\n\n        optimizer.zero_grad()\n        outputs = model(input_ids, attention_mask, token_type_ids)\n        loss = criterion(outputs, targets)\n        loss.backward()\n        optimizer.step()\n\n        epoch_loss += loss.item()\n        _, predicted = torch.max(outputs, 1)\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\n\n    accuracy = correct / total\n    print(f'Epoch {epoch + 1}/{EPOCHS}, Loss: {epoch_loss / len(train_loader)}, Accuracy: {accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-22T06:07:47.499884Z","iopub.execute_input":"2024-05-22T06:07:47.500452Z","iopub.status.idle":"2024-05-22T08:33:11.957680Z","shell.execute_reply.started":"2024-05-22T06:07:47.500423Z","shell.execute_reply":"2024-05-22T08:33:11.956666Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Epoch 1/30, Loss: 0.6570495637220757, Accuracy: 0.5971896955503513\nEpoch 2/30, Loss: 0.5191864074669151, Accuracy: 0.7555620608899297\nEpoch 3/30, Loss: 0.390770684057307, Accuracy: 0.8348946135831382\nEpoch 4/30, Loss: 0.3294785905838291, Accuracy: 0.8665105386416861\nEpoch 5/30, Loss: 0.27304428218750754, Accuracy: 0.900175644028103\nEpoch 6/30, Loss: 0.21445917289390742, Accuracy: 0.9247658079625293\nEpoch 7/30, Loss: 0.16892693020298938, Accuracy: 0.9417447306791569\nEpoch 8/30, Loss: 0.14579015245191126, Accuracy: 0.9487704918032787\nEpoch 9/30, Loss: 0.10696816187195271, Accuracy: 0.9625292740046838\nEpoch 10/30, Loss: 0.08286511351650354, Accuracy: 0.969847775175644\nEpoch 11/30, Loss: 0.062353569989214506, Accuracy: 0.9795081967213115\nEpoch 12/30, Loss: 0.051820495944887954, Accuracy: 0.9815573770491803\nEpoch 13/30, Loss: 0.043238260986709916, Accuracy: 0.9879976580796253\nEpoch 14/30, Loss: 0.031742346910411647, Accuracy: 0.9909250585480094\nEpoch 15/30, Loss: 0.030438396984506746, Accuracy: 0.9897540983606558\nEpoch 16/30, Loss: 0.05389197736980707, Accuracy: 0.9838992974238876\nEpoch 17/30, Loss: 0.055353965449786664, Accuracy: 0.9815573770491803\nEpoch 18/30, Loss: 0.03926846703403481, Accuracy: 0.9874121779859485\nEpoch 19/30, Loss: 0.03393837436136059, Accuracy: 0.9909250585480094\nEpoch 20/30, Loss: 0.027700474050557502, Accuracy: 0.9929742388758782\nEpoch 21/30, Loss: 0.02500880896070293, Accuracy: 0.9944379391100703\nEpoch 22/30, Loss: 0.024547234062806084, Accuracy: 0.9935597189695551\nEpoch 23/30, Loss: 0.027155117100489838, Accuracy: 0.9915105386416861\nEpoch 24/30, Loss: 0.04541854211319647, Accuracy: 0.9853629976580797\nEpoch 25/30, Loss: 0.02746769514195425, Accuracy: 0.9918032786885246\nEpoch 26/30, Loss: 0.01992610575281471, Accuracy: 0.9953161592505855\nEpoch 27/30, Loss: 0.014183142349487345, Accuracy: 0.9961943793911007\nEpoch 28/30, Loss: 0.008550415076391562, Accuracy: 0.9982435597189696\nEpoch 29/30, Loss: 0.007165121611914376, Accuracy: 0.9985362997658079\nEpoch 30/30, Loss: 0.006951203600097683, Accuracy: 0.9979508196721312\n","output_type":"stream"}]},{"cell_type":"code","source":"#Define the path where the model will be saved\nmodel_save_path = \"/kaggle/working/stacked_transformer_model.pth\"\n\n# Save the model\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Define the path where the model is saved\nmodel_load_path = \"/kaggle/working/stacked_transformer_model.pth\"\n\n# Recreate the model architecture\nloaded_model = StackedTransformer(num_layers=NUM_LAYERS, pretrained_model_name='roberta-base', num_classes=NUM_CLASSES, hidden_size=HIDDEN_SIZE)\n\n# Load the state dictionary into the model\nloaded_model.load_state_dict(torch.load(model_load_path))\nloaded_model = loaded_model.to(device)\nprint(\"Model loaded successfully\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loaded_model.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        targets = batch['targets'].to(device, dtype=torch.long)\n\n        outputs = loaded_model(input_ids, attention_mask, token_type_ids)\n        \n        if outputs.dim() == 1:\n            outputs = outputs.unsqueeze(0)\n        \n        if outputs.dim() == 2:\n            _, predicted = torch.max(outputs, 1)\n        else:\n            raise ValueError(f'Unexpected output shape: {outputs.shape}')\n\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\n\n    accuracy = correct / total\n    print(f'Accuracy: {accuracy}')\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prediction function remains the same\ndef predict_sentiment(text, model, tokenizer, max_len):\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            truncation=True,\n            return_token_type_ids=True\n        )\n        input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0).to(device)\n        attention_mask = torch.tensor(inputs['attention_mask']).unsqueeze(0).to(device)\n        token_type_ids = torch.tensor(inputs[\"token_type_ids\"]).unsqueeze(0).to(device)\n        outputs = model(input_ids, attention_mask, token_type_ids)\n        \n        outputs = outputs.view(1, -1)\n        probabilities = torch.softmax(outputs, dim=1)\n        _, predicted_index = torch.max(probabilities, 1)\n        predicted_label = \"Positive\" if predicted_index.item() == 1 else \"Negative\"\n        return predicted_label\n\nnew_text = [\n    \"You have to go down to the hamlets without your entourage, hey. We'll show you... Many people who are in poverty... Not from now on, but for a long time... People have a vote bank. Come up with a rule that those above the age of 50 should not be made ministers. There are many knowledgeable and educated young people in Kerala who are capable of holding the wheel of governance... There is no need for any political legacy... We'll rule and show you. A poverty-free, waste-free Kerala. It's like it's got alfaham in Koren's hand now...A multi fungal virus in Kerala Poli- Trics...\",\n    \"What is that Plan B, sir? Let the surprise come out, sir. Are you going to shave off everything you've taken away and leave the country? Or is it to be declared a bankrupt? Or is it that al-Kerala will establish the kingdom? With all this in mind, the NIA is building a massive office in Kochi. If there is a raid that started in Kochi today, it is the first step to demolish this Plan B... Beware...\",\n    \"The people who witnessed the incident told the truth, so others understand that he was innocent.\"]\n\npredicted_sentiments = []\nfor tweet in new_text:\n    predicted_sentiment = predict_sentiment(tweet, loaded_model, tokenizer, MAX_LEN)\n    predicted_sentiments.append(predicted_sentiment)\n\nfor tweet, sentiment in zip(new_text, predicted_sentiments):\n    print(f\"{tweet}  :  {sentiment}\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluation\nmodel.eval()\nwith torch.no_grad():\n    correct = 0\n    total = 0\n    for batch in test_loader:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        token_type_ids = batch['token_type_ids'].to(device)\n        targets = batch['targets'].to(device, dtype=torch.long)\n\n        outputs = model(input_ids, attention_mask, token_type_ids)\n        \n\n        # Ensure outputs have the expected shape\n        if outputs.dim() == 1:\n            # Handle the case where outputs might be a single-dimension tensor\n            outputs = outputs.unsqueeze(0)  # Add a batch dimension if needed\n        \n        if outputs.dim() == 2:\n            _, predicted = torch.max(outputs, 1)\n        else:\n            raise ValueError(f'Unexpected output shape: {outputs.shape}')\n\n        total += targets.size(0)\n        correct += (predicted == targets).sum().item()\n\n    accuracy = correct / total\n    print(f'Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-05-22T08:33:11.960362Z","iopub.execute_input":"2024-05-22T08:33:11.960702Z","iopub.status.idle":"2024-05-22T08:33:34.552255Z","shell.execute_reply.started":"2024-05-22T08:33:11.960674Z","shell.execute_reply":"2024-05-22T08:33:34.551316Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Accuracy: 0.8395784543325527\n","output_type":"stream"}]},{"cell_type":"code","source":"# Prediction function remains the same\ndef predict_sentiment(text, model, tokenizer, max_len):\n    model.eval()\n    with torch.no_grad():\n        inputs = tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=max_len,\n            padding='max_length',\n            truncation=True,\n            return_token_type_ids=True\n        )\n        input_ids = torch.tensor(inputs['input_ids']).unsqueeze(0).to(device)\n        attention_mask = torch.tensor(inputs['attention_mask']).unsqueeze(0).to(device)\n        token_type_ids = torch.tensor(inputs[\"token_type_ids\"]).unsqueeze(0).to(device)\n        outputs = model(input_ids, attention_mask, token_type_ids)\n        \n        # Reshape outputs tensor\n        outputs = outputs.view(1, -1)\n        probabilities = torch.softmax(outputs, dim=1)\n        _, predicted_index = torch.max(probabilities, 1)\n        predicted_label = \"Positive\" if predicted_index.item() == 1 else \"Negative\"\n        return predicted_label\n\n\nnew_text = [\n    \"* I'm not the smartest guy and i hardly know everything, but i have always been a keen student history and a follower of politics, particularly US politics for 30 years. What is chilling, and believe me i did get the chills while watching this video, is that I 100% agree with his analysis. He vocalized succinctly what has been causing me great anxiety deep down for a long time about US and global politics. An anxiety that has been affecting me in a way it has never affected me before. Too many people underestimate the very real potential threat Trump and his ilk pose to not only US, but also global stability. Quite frankly it is downright frightening. No joke. And all of it is the result of what the state of the U.S. political system, among other things, has been allowed to degrade to over the last few decades. Of course it was never perfect, and there have been many tense global events in the decades since WW2, but now we really are on a real precipice looking over the edge into the abyss like never before. Yes, some might call it 'Trump delusion syndrome', or that i am exaggerating, but honestly, deep down, after much self reflection....i don't think i am. Let's just hope the proverbial coin flip lands right side up, however that needs to play out.\",\n    \"* Indian policy consider leader nation representative. Right Biden president represents country. Personal friendships room politics.\",\n    \"* This needs to be a criminal offence. As does lying to the public if an MP or a news outlet.\",\n    \"* Quite frankly it is downright frightening. No joke.\",\n    \"* The people who witnessed the incident told the truth, so others understand that he was innocent.\",\n    \"* It result policies party followed powerful party country become nothing Indian politics today. Only party good country people survive, whichever party is.\",\n    \"* The very objective of the Sangh Parivar is to create communal riots across the country.......yet Satheesan and his gang don't understand!Babri Masjid has disappeared as a result of these people's silence! It is true that minorities should vote carefully when the election is near. Because if the Sunkis come to power, the country itself will be brought to Kavi! It will be dangerous! What the Chief Minister said is correct. What is the doubt about that.....\",\n    \"* Yes the former president of the Philippines at one time jokingly said that he wanted the Philippines to become one of the provinces of china. Many Filipinos believe that if the dutertes will have a chance to regain power the Philippines will become province of china.\",\n    \"* Too many people underestimate the very real potential threat Trump and his ilk pose to not only US, but also global stability. Quite frankly it is downright frightening. No joke. And all of it is the result of what the state of the U.S. political system, among other things, has been allowed to degrade to over the last few decades. Of course it was never perfect, and there have been many tense global events in the decades since WW2, but now we really are on a real precipice looking over the edge into the abyss like never before. Yes, some might call it 'Trump delusion syndrome', or that i am exaggerating, but honestly, deep down, after much self reflection....i don't think i am. Let's just hope the proverbial coin flip lands right side up, however that needs to play out.\",\n    \"* The coordinated naval exercises in the South China Sea demonstrate unity and commitment to international norms, which is commendable. They uphold freedom of navigation, a vital principle for global trade and security. However, while these actions aim to deter unilateral aggression and promote stability, there's a risk of escalating tensions in the region. The involvement of multiple naval forces increases the potential for accidents and unintended conflicts. Despite the importance of regional partnerships for enhancing peace and security in the Asia-Pacific, the militarization of the area could hinder diplomatic efforts for peaceful resolutions. Therefore, while military readiness is essential, it must be balanced with diplomatic dialogue to prevent further escalation and ensure a peaceful resolution to the disputes in the South China Sea\",\n    \"* The coordinated naval exercises in the South China Sea demonstrate unity and commitment to international norms, which is commendable.\",\n    \"* They uphold freedom of navigation, a vital principle for global trade and security.\",\n    \"* The actions aim to deter unilateral aggression and promote stability.\", \n    \"* There's a risk of escalating tensions in the region.\",\n    \"* The potential for accidents and unintended conflicts are high due to involvement of multiple naval forces.\",\n    \"* Concern about the hindrance of diplomatic efforts by the militarization of the area enhancing peace and security in the Asia-Pacific\",\n    \"* The militarization of the area could hinder diplomatic efforts for peaceful resolutions \", \n    \"* Military readiness is essential, it must be balanced with diplomatic dialogue to prevent further escalation and ensure a peaceful resolution to the disputes in the South China Sea\",\n    \"* The government's new climate change policy demonstrates a willingness to address environmental issues, which is necessary.\",\n    \"* Investing in renewable energy and setting emissions targets are positive moves.\",\n    \"* The policy lacks the aggressive measures needed to mitigate the rapid effects of climate change.\",\n    \"* The intention is commendable, the policy falls short in enforcing stringent regulations and providing clear pathways for significant, immediate reductions in greenhouse gases.\" ,\n    \"* The reliance on voluntary compliance from industries raises doubts about its effectiveness.\",\n    \"* This could lead to continued environmental degradation and missed opportunities to avert more severe climate impacts.\",\n    \"* The government's new climate change policy demonstrates a willingness to address environmental issues, which is necessary. Investing in renewable energy and setting emissions targets are positive moves. Nevertheless, the policy lacks the aggressive measures needed to mitigate the rapid effects of climate change. The reliance on voluntary compliance from industries raises doubts about its effectiveness. While the intention is commendable, the policy falls short in enforcing stringent regulations and providing clear pathways for significant, immediate reductions in greenhouse gases. This could lead to continued environmental degradation and missed opportunities to avert more severe climate impacts.\",\n    \"* The unpredictability of Trump?  Really?  Simple.  He's gonna do what he says  he's gonna do, it will be in the best interest of the USA,  and whatever he does, it will have the focus of making America great!  Low gas prices, thriving economy, NO NEW WARS, peace treaties, fight the deep set special interests, finished boarder wall.. I WISH we had a politician with his fortitude here in Canada.  TRUMP 2024!!!\"]\npredicted_sentiments = []\nfor tweet in new_text:\n    predicted_sentiment = predict_sentiment(tweet, model, tokenizer, MAX_LEN)\n    predicted_sentiments.append(predicted_sentiment)\n\nfor tweet, sentiment in zip(new_text, predicted_sentiments):\n    print(f\"{tweet}  :  {sentiment}\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-05-22T10:07:08.558666Z","iopub.execute_input":"2024-05-22T10:07:08.559043Z","iopub.status.idle":"2024-05-22T10:07:09.734198Z","shell.execute_reply.started":"2024-05-22T10:07:08.559014Z","shell.execute_reply":"2024-05-22T10:07:09.733225Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"* I'm not the smartest guy and i hardly know everything, but i have always been a keen student history and a follower of politics, particularly US politics for 30 years. What is chilling, and believe me i did get the chills while watching this video, is that I 100% agree with his analysis. He vocalized succinctly what has been causing me great anxiety deep down for a long time about US and global politics. An anxiety that has been affecting me in a way it has never affected me before. Too many people underestimate the very real potential threat Trump and his ilk pose to not only US, but also global stability. Quite frankly it is downright frightening. No joke. And all of it is the result of what the state of the U.S. political system, among other things, has been allowed to degrade to over the last few decades. Of course it was never perfect, and there have been many tense global events in the decades since WW2, but now we really are on a real precipice looking over the edge into the abyss like never before. Yes, some might call it 'Trump delusion syndrome', or that i am exaggerating, but honestly, deep down, after much self reflection....i don't think i am. Let's just hope the proverbial coin flip lands right side up, however that needs to play out.  :  Positive\n\n* Indian policy consider leader nation representative. Right Biden president represents country. Personal friendships room politics.  :  Positive\n\n* This needs to be a criminal offence. As does lying to the public if an MP or a news outlet.  :  Negative\n\n* * Quite frankly it is downright frightening. No joke.  :  Negative\n\n* The people who witnessed the incident told the truth, so others understand that he was innocent.  :  Positive\n\n* It result policies party followed powerful party country become nothing Indian politics today. Only party good country people survive, whichever party is.  :  Positive\n\n* The very objective of the Sangh Parivar is to create communal riots across the country.......yet Satheesan and his gang don't understand!Babri Masjid has disappeared as a result of these people's silence! It is true that minorities should vote carefully when the election is near. Because if the Sunkis come to power, the country itself will be brought to Kavi! It will be dangerous! What the Chief Minister said is correct. What is the doubt about that.....  :  Negative\n\n* Yes the former president of the Philippines at one time jokingly said that he wanted the Philippines to become one of the provinces of china. Many Filipinos believe that if the dutertes will have a chance to regain power the Philippines will become province of china.  :  Positive\n\n* Too many people underestimate the very real potential threat Trump and his ilk pose to not only US, but also global stability. Quite frankly it is downright frightening. No joke. And all of it is the result of what the state of the U.S. political system, among other things, has been allowed to degrade to over the last few decades. Of course it was never perfect, and there have been many tense global events in the decades since WW2, but now we really are on a real precipice looking over the edge into the abyss like never before. Yes, some might call it 'Trump delusion syndrome', or that i am exaggerating, but honestly, deep down, after much self reflection....i don't think i am. Let's just hope the proverbial coin flip lands right side up, however that needs to play out.  :  Positive\n\n* * The coordinated naval exercises in the South China Sea demonstrate unity and commitment to international norms, which is commendable. They uphold freedom of navigation, a vital principle for global trade and security. However, while these actions aim to deter unilateral aggression and promote stability, there's a risk of escalating tensions in the region. The involvement of multiple naval forces increases the potential for accidents and unintended conflicts. Despite the importance of regional partnerships for enhancing peace and security in the Asia-Pacific, the militarization of the area could hinder diplomatic efforts for peaceful resolutions. Therefore, while military readiness is essential, it must be balanced with diplomatic dialogue to prevent further escalation and ensure a peaceful resolution to the disputes in the South China Sea  :  Positive\n\n* The coordinated naval exercises in the South China Sea demonstrate unity and commitment to international norms, which is commendable.  :  Positive\n\n* They uphold freedom of navigation, a vital principle for global trade and security.  :  Positive\n\n* The actions aim to deter unilateral aggression and promote stability.  :  Positive\n\n* There's a risk of escalating tensions in the region.  :  Negative\n\n* The potential for accidents and unintended conflicts are high due to involvement of multiple naval forces.  :  Positive\n\n* Concern about the hindrance of diplomatic efforts by the militarization of the area enhancing peace and security in the Asia-Pacific  :  Negative\n\n* The militarization of the area could hinder diplomatic efforts for peaceful resolutions   :  Positive\n\n* Military readiness is essential, it must be balanced with diplomatic dialogue to prevent further escalation and ensure a peaceful resolution to the disputes in the South China Sea  :  Positive\n\n* The government's new climate change policy demonstrates a willingness to address environmental issues, which is necessary.  :  Positive\n\n* Investing in renewable energy and setting emissions targets are positive moves.  :  Positive\n\n* The policy lacks the aggressive measures needed to mitigate the rapid effects of climate change.  :  Negative\n\n* The intention is commendable, the policy falls short in enforcing stringent regulations and providing clear pathways for significant, immediate reductions in greenhouse gases.  :  Positive\n\n* The reliance on voluntary compliance from industries raises doubts about its effectiveness.  :  Negative\n\n* This could lead to continued environmental degradation and missed opportunities to avert more severe climate impacts.  :  Negative\n\n* The government's new climate change policy demonstrates a willingness to address environmental issues, which is necessary. Investing in renewable energy and setting emissions targets are positive moves. Nevertheless, the policy lacks the aggressive measures needed to mitigate the rapid effects of climate change. The reliance on voluntary compliance from industries raises doubts about its effectiveness. While the intention is commendable, the policy falls short in enforcing stringent regulations and providing clear pathways for significant, immediate reductions in greenhouse gases. This could lead to continued environmental degradation and missed opportunities to avert more severe climate impacts.  :  Positive\n\n","output_type":"stream"}]},{"cell_type":"code","source":"print(accuracy)","metadata":{"execution":{"iopub.status.busy":"2024-05-22T09:55:52.754004Z","iopub.execute_input":"2024-05-22T09:55:52.754693Z","iopub.status.idle":"2024-05-22T09:55:52.759217Z","shell.execute_reply.started":"2024-05-22T09:55:52.754660Z","shell.execute_reply":"2024-05-22T09:55:52.758356Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"0.8395784543325527\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}